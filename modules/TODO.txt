TODO: 

*   Work on plan to consolidate token_defs and grammar
*   Work on removing hard coded implemenation details from dlexer.py and lexer.py: 
        > Currently, to add NEW tokens to support new items in the grammar I have to:
            > Add the token def to the token_defs.txt file
            > Add the token name as an int to the dlexer.py file (only for autocomplete tho I think?)
            > If it's a multichar token, add it to the lexer method in lexer.py

        > I don't like that, it should all be generated from the grammar file.

*   Write tests for parser: Ensure it correctly parses everything in your grammar
    Consider using pytest? Or some other testing framework.


*  Bridge the gap between d_lexer.py and parser_generator.py in regards to generating lexer methods from the grammar file.
   Currently, we load the SOQ and MB funcs from d_lexer. 
   I'd like it if, as the parser, we read the grammar file, saw the multi-char token defs, and generated funcs to send to 
   the lexer to initilize it.

   We have funcs in d_lexer currently, and we have the ability to parse the multi-char token defs in the parser_generator.
   We just have to create a way to generate the functions from the list of nodes that we have. 



